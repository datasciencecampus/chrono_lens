# Acceptance Tests for `process_scheduled`

These tests can be run manually to confirm that the code is working as required. Test confirms the following cloud
functions are working:

* `process_scheduled`
* `run_model_on_image`

Note that this is a full system test so will need to live download images, and to verify this will need an empty
database (i.e. to simplify testing), so needs a scratch project ideally. Tests can also be carried out non-destructively,
but are more complex.

Note that `${PROJECT_ID}` is meant to represent the identifying name of your current Google Compute Platform project,
and is defined as part of `gcp-setup.sh`

## Direct trigger - destructive test

This confirms that the environment variables are correctly set as well as the download functionality scales
correctly, and does not suffer from timeouts.

### Test Steps

1. Ensure all Cloud Scheduler jobs are paused.

1. Empty the bucket `data-${PROJECT_ID}` to ensure any downloaded files are easily identified.

1. Run the `update-sources` cloud function to ensure the `sources-${PROJECT_ID}` is up-to-date.

1. Upload a configuration file for scheduled analysis to the root of `sources-${PROJECT_ID}`, such as
`analyse-configuration.json` in the `processed_scheduled/exampleJSON` folder.

1. Upload an appropriate image source JSON file to the `analyse` folder of `sources-${PROJECT_ID}`, such
as `Durham-images.json` in the `processed_scheduled/exampleJSON` folder.

1. Start the Cloud Scheduler job "every-10mins".

### Expected Outcome

1. After 10 minutes, you will see:
  * First group of images will be downloaded, to the `data-${PROJECT_ID}` bucket.
  * A new table in BigQuery within `${PROJECT_ID}`'s `detected_objects` dataset; name will be that of
    the model named in `sources-${PROJECT_ID}` file `analyse-configuration.json` (such as
    "FaultyImageFilterV0_NewcastleV0_StaticObjectFilterV0")
  * All entries in the table will be flagged as "missing" as the images are processed 20 minutes in the past

1. After 20 minutes, you will see:
  * Second group of images will be downloaded, to the `data-${PROJECT_ID}` bucket.
  * The BigQuery table within `${PROJECT_ID}`'s `detected_objects` dataset will be updated.
  * All entries in the table will be flagged as "missing" as the images are processed 20 minutes in the past.

1. After 30 minutes, you will see:
  * Third group of images will be downloaded, to the `data-${PROJECT_ID}` bucket.
  * The BigQuery table within `${PROJECT_ID}`'s `detected_objects` dataset will be updated.
  * The latest entries in the table will contain analysis results, as there are now sufficient images to process.
  * Note that analysis will need a previous, current and next image if faulty image filtering is used; in which case,
    the results will not find a "previous" image instead filtering only with "next" image as the "current" image will
    be th first image in the sequence.

1. After 40 minutes, you will see:
  * Fourth group of images will be downloaded, to the `data-${PROJECT_ID}` bucket.
  * The BigQuery table within `${PROJECT_ID}`'s `detected_objects` dataset will be updated.
  * The latest entries in the table will contain analysis results, as there are now sufficient images to process.
  * Any image analysis requiring a previous, current and next image will now operate at full capacity as all required
    images are now available.


### Clean up

1. You may wish to delete the folders from bucket `data-${PROJECT_ID}` unless they are of further use.

1. You may wish to delete the table from BigQuery `${PROJECT_ID}` unless it is of further use.


## Non-destructive test variants

You can run the trigger-based or direct function based tests without first emptying the bucket,
but you will have to navigate to each sub-folder that represents the time the test was triggered,
to discover how many images are present. Manual clean-up (removal of these test downloads) will also
be more work (however, just ask to delete each unique time folder generated by the test to
determine the number of images downloaded, then confirm the deletion).

It is recommended to trigger the function safely away from 10 minute multiples past the hour
(so avoid :00, :10, :20 etc.) so you can identify the files generated by your test rather than the
scheduled 10 minute trigger.
